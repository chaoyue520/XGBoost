#!/usr/bin/python
# -*- coding: utf-8 -*-
import pandas as pd
import numpy as np
import xgboost as xgb
import sys
import gc
import re

reload(sys)
sys.setdefaultencoding('utf8')

#记录程序运行时间
import time 
start_time = time.time()


# read data 

na_values=['','NULL','NA','null','na','Na','-9999','-1','Infinity','NaN']

data_set = pd.read_table('/home/fsg/jiwenchao/ac_class/data/data_set_0705.txt',sep = '\t',na_values = na_values)


# split train valid oot dataset 
train_set = data_set[(data_set.data_set_tag == 1)]
valid_set = data_set[(data_set.data_set_tag == 2)]
oot_set = data_set[(data_set.data_set_tag == 3)]


#概览数据
data_set['risk_tag_union'].value_counts()
train_set.groupby(['risk_tag_union'])['passid'].count()
valid_set.groupby(['risk_tag_union'])['passid'].count()
oot_set.groupby(['risk_tag_union'])['passid'].count()


# selected model vars 
col_x_old = list(data_set.columns)

#临时删除关键字段
remove_vars=['passid','sessionid','data_set_tag'
,'sessionid.1','risk_tag_union','paytype_credit_pay','uid_base_payamt']


#只保留最终进入模型的变量
col_x = [x for x in col_x_old if x not in remove_vars]

#逻辑判断
len(col_x) == len(col_x_old)-len(remove_vars)


#释放内存
del data_set
print gc.collect


# to DMatrix
train_data = train_set[col_x].as_matrix()
train_label = train_set['risk_tag_union'].as_matrix()
dtrain = xgb.DMatrix(train_data, label = train_label)

valid_data = valid_set[col_x].as_matrix()
valid_label = valid_set['risk_tag_union'].as_matrix()
dvalid = xgb.DMatrix(valid_data, label = valid_label)

oot_data = oot_set[col_x].as_matrix()
oot_label = oot_set['risk_tag_union'].as_matrix()
doot = xgb.DMatrix(oot_data, label = oot_label)


#模型数据分配
evallist = [(dtrain, 'train'), (dvalid, 'valid'), (doot, 'oot')]


# parameters
param = { 'objective': 'binary:logistic',    #定义学习任务及相应的学习目标
          'eval_metric': 'auc',      #校验数据所需要的评价指标
          'max_depth': 3,            #避免过拟合，过大则会学到更多的局部特征，易导致过拟合
          'learning_rate': 0.01,     #可用来防止过拟合，eta，学习速率，更新过程中用到的收缩步长
          'min_child_weight': 50,   #子节点中最小的样本权重和，调高可以避免过拟合，越大算法越conservative
          'silent': 1,               #取1时表示以缄默方式运行，不打印运行时信息，取0时表示打印出运行时信息。
          #'lambda':1,               #L2正则化权重，减少过拟合
          #'alpha': 1,               #L1正则化权重，减少过拟合 
          'gamma': 0.8,             #值越大，算法越保守
          'max_delta_step': 1,       #限制每棵树权重改变的最大步长
          #'subsample': 0.95,         #避免过拟合，但是如果过小，则易导致欠拟合，子样本占整个样本集合的比例，可防止过拟合
          #'colsample_bytree': 0.95,  #避免过拟合，但是如果过小，则易导致欠拟合，在建立树时对特征采样的比例，可防止过拟合
          #'scale_pos_weight': 1,    #类别十分不平衡
          'seed': 1                  #随机数的种子。缺省值为0
          }


# 迭代次数 the max number of iterations
num_rounds=950



# 释放内存
del data_set
del train_data
del valid_data
del oot_data

gc.set_debug(gc.DEBUG_STATS|gc.DEBUG_LEAK)
print gc.collect()


#tunning the model
bst = xgb.train(param,dtrain,num_boost_round = num_rounds , evals = evallist)


print "best best_ntree_limit:",bst.best_ntree_limit 

#输出运行时间
endtime = time.time()
cost_time = endtime - start_time
print "xgboost success!",'\n',"cost time:",cost_time,"(s)"

############################################################################ load model ##############################################################################
######## 思考：save_model和dump_model的区别
#1、Both functions save_model and dump_model save the model, the difference is that in dump_model you can save feature name and save tree in text format.
#2、The load_model will work with model from save_model.


#1、save model 
xpath='/home/fsg/jiwenchao/ac_class/data'
model_name='/01_ac_class_fraud.model'

bst.save_model(xpath+model_name)

